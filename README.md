Pre-request:
Pull and run Ollama models locally:
  1, brew install --cask ollama
  2, ollama pull llama3.1:8b
  3, ollama pull nomic-embed-text
  4, ollama list
  5, ollama serve &
  6, curl -s http://127.0.0.1:11434 | head
  
Run the project in the entry point.
